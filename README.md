# Schedule-Free Muon: Removing Hyperparameters from Matrix-Based Learning

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)

> **PhD Research Project**: A geometrically-aware optimizer that combines Schedule-Free Learning with Muon's spectral updates, eliminating the need for learning rate schedules in deep neural networks.

---

## üéØ Executive Summary

### The Problem

Modern deep learning optimizers face a critical hyperparameter dilemma:

1. **AdamW** requires carefully tuned learning rate schedules (warmup + cosine decay)
2. Choosing the wrong total training duration (`T`) leads to suboptimal convergence
3. **Muon** (a matrix-aware optimizer) achieves faster convergence but still needs manual schedules

**Real-World Impact**: If you train for 100 epochs but stop at epoch 50, your model is suboptimal because the schedule "didn't land yet." If you extend to 200 epochs, you must restart with a re-scaled schedule.

### Our Solution

**Schedule-Free Muon** unifies two cutting-edge techniques:

| Component | What It Does | Benefit |
|-----------|--------------|---------|
| **Schedule-Free Learning** (Defazio et al., 2024) | Replaces LR decay with primal-dual averaging | ‚úÖ No need to specify training duration `T` |
| **Muon Optimizer** (Jordan et al., 2024) | Orthogonal updates for matrix parameters | ‚úÖ Spectral conditioning (convergence speed validation pending) |
| **Our Innovation** | Deferred manifold projection | ‚úÖ Per-step projection of training weights, high-precision correction at inference |

**Key Innovation**: We address the fundamental incompatibility between:
- Schedule-Free's **Euclidean averaging** (assumes flat space)
- Muon's **Riemannian geometry** (updates on manifold of orthogonal matrices)

Our solution: **Continuous projection** of training weights back to the Stiefel manifold at every step.

---

## üöÄ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/your-org/schedule-free-muon.git
cd schedule-free-muon

# Install dependencies
pip install -r requirements.txt
```

### Basic Usage

```python
import torch
import torch.nn as nn
from sf_muon import ScheduleFreeMuon

# 1. Define your model
model = nn.TransformerEncoder(...)

# 2. Initialize optimizer (no schedule needed!)
optimizer = ScheduleFreeMuon(
    model.parameters(),
    lr=0.02,           # Aggressive LR (Muon-style)
    weight_decay=0.01
)

# 3. Training loop
for epoch in range(num_epochs):  # No need to know num_epochs in advance!
    for batch in dataloader:
        optimizer.zero_grad()
        loss = model(batch)
        loss.backward()
        optimizer.step()

# 4. Before inference (CRITICAL!)
optimizer.normalize_averaged_weights()
model.eval()
test_loss = evaluate(model, test_loader)
```

### Run Tests

```bash
# Geometric stress test (validates manifold recovery)
python stress_test_manifold.py

# Full validation suite
python main.py --test all
```

---

## üìä Experimental Validation

### Test 1: Manifold Recovery (Geometric Soundness)

**Setup**: Single linear layer (1024√ó1024) initialized with Gaussian noise  
**Task**: Regress to orthogonal target matrix  
**Metric**: Spectral error `||W^T W - I||_F`

**Results**:
| Phase | Spectral Error | Interpretation |
|-------|----------------|----------------|
| Initial (random) | 23.02 | Far from manifold (expected) |
| During training | ~31.89 | Drift in averaged buffer (expected) |
| After inference projection | 0.000042 | Machine precision ‚úÖ |

**Key Finding**: ‚úÖ The optimizer successfully projects accumulated averages 
to the Stiefel manifold at inference time, achieving machine precision.

**Note on Training Error**: The elevated error during training (~31.89) reflects 
drift in the averaged buffer `y` from Euclidean averaging. This is the fundamental 
trade-off of combining Schedule-Free (Euclidean) with Muon (Riemannian). The 
training weights `p` used for forward passes are projected each step, while full 
correction of `y` is deferred to inference for computational efficiency.

### Test 2: Computational Cost (CPU Benchmark)

**Configuration**: 4-layer network, d_model=1024, CPU

| Optimizer | Steps/sec | Overhead | Notes |
|-----------|-----------|----------|-------|
| AdamW | 0.93 | 1√ó | Baseline |
| SF-Muon | 0.69 | 1.34√ó | **CPU-specific** |

**Critical Caveats**:
‚ö†Ô∏è **This 1.34√ó overhead is CPU-specific and NOT representative of GPU performance.**

Theoretical analysis predicts **10-50√ó overhead** on GPU due to O(N¬≥) Newton-Schulz 
iterations. The low CPU overhead likely reflects:
- Memory bandwidth bottleneck (not compute-bound)
- Small matrix size (1024√ó1024)
- CPU-specific compiler optimizations

**GPU validation with large matrices (4096√ó4096) is required** before making 
performance claims.

### Test 3: Convergence Quality

**Setup**: Synthetic regression task  
**Result**: Final test loss = 0.0079 ‚úÖ

The optimizer successfully minimizes loss while maintaining geometric constraints.
Comparison to AdamW baseline on real datasets (WikiText-2) is future work.

---

## üßÆ Mathematical Foundation

### The Core Challenge

Schedule-Free Learning guarantees convergence via:
$$
y_{t+1} = (1 - c_k) y_t + c_k z_{t+1}, \quad c_k = \frac{1}{k+1}
$$

**Problem**: For orthogonal matrices $W \in O(n)$, this linear averaging produces:
$$
y_{t+1} \notin O(n) \quad \text{(leaves the manifold!)}
$$

**Proof by Counter-Example**:
```
Let y_t = I, z_t = -I (both orthogonal)
Then y_{t+1} = 0.5*I + 0.5*(-I) = 0 (singular matrix!)
```

### Our Solution: Projected Averaging

**Training Weights** (used for forward pass):
$$
W_{\text{train}} = \text{NewtonSchulz}\left[(1-s_t)y_t + s_t z_t\right]
$$

**Inference Weights** (final projection):
$$
W_{\text{final}} = \text{NewtonSchulz}(y_T, \text{steps}=10)
$$

**Newton-Schulz Iteration** (Quintic, 5th-order convergence):
$$
X_{k+1} = aX_k + bX_k(X_k^T X_k) + cX_k(X_k^T X_k)^2
$$

Where $(a, b, c) = (3.4445, -4.7750, 2.0315)$ are optimized coefficients.

---

## üî¨ Implementation Details

### Architecture

```
schedule-free-muon/
‚îú‚îÄ‚îÄ sf_muon.py                 # Core optimizer
‚îÇ   ‚îú‚îÄ‚îÄ quintic_newton_schulz()  # Orthogonalization kernel
‚îÇ   ‚îî‚îÄ‚îÄ ScheduleFreeMuon         # Main optimizer class
‚îú‚îÄ‚îÄ stress_test_manifold.py    # Geometric validation
‚îú‚îÄ‚îÄ main.py                     # Comprehensive test suite
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

### Key Design Decisions

1. **Memory Efficiency**: All operations in-place, no persistent clones
   ```python
   # ‚ùå Bad: Allocates temporary tensor
   p_combined = y.clone().mul_(1 - sched).add_(z, alpha=sched)
   
   # ‚úÖ Good: In-place operations
   p.data.copy_(y)
   p.data.mul_(1 - sched).add_(z, alpha=sched)
   ```

2. **Layer Selectivity**: Automatic routing based on tensor dimension
   - `ndim < 2` ‚Üí AdamW (vectors: bias, LayerNorm)
   - `ndim >= 2` ‚Üí Muon (matrices: Linear, Conv2D)

3. **Periodic Anchor Correction**: Every 100 steps, re-project anchor `z`
   ```python
   if step % 100 == 0:
       z = quintic_newton_schulz(z, steps=1)
   ```

### Computational Cost Breakdown

For matrix $W \in \mathbb{R}^{N \times N}$:

| Operation | Cost | Frequency |
|-----------|------|-----------|
| Gradient orthogonalization | $40N^3$ FLOPs | Every step |
| Forward pass projection | $8N^3$ FLOPs | Every step |
| Anchor correction | $8N^3$ FLOPs | Every 100 steps |
| **Total per step** | **$\approx 48N^3$** | - |
| AdamW (baseline) | $4N^2$ FLOPs | Every step |
| **Slowdown** | **$12N$** | For $N=4096$: ~50√ó |

---

## ‚ö†Ô∏è Limitations & Future Work

### Current Limitations

1. **Computational Overhead**: 
   - **Theoretical**: 10-50√ó slower per step than AdamW (O(N¬≥) vs O(N¬≤))
   - **Measured (CPU)**: 1.34√ó slower (likely bandwidth-limited, not compute-bound)
   - **GPU validation pending**: True overhead unknown until tested on large-scale GPU training

2. **Training Dynamics**:
   - Spectral error remains elevated (~32) during training
   - Only corrected at inference via high-precision projection
   - Long-term effects of drift accumulation unexplored

3. **Theoretical Gap**: 
   - Convergence proof incomplete for projected averaging
   - Bias from inference-time projection not formally bounded
   - Approximation of geodesic averaging, not true Riemannian method

4. **Experimental Validation**: 
   - ‚ùå No multi-layer transformer experiments
   - ‚ùå No comparison to AdamW on real datasets
   - ‚ùå No large-scale LLM training validation
   - ‚úÖ Geometric soundness proven on synthetic tasks

### Planned Improvements

- [ ] **CUDA Kernel Fusion**: Custom kernel for NS iteration (2-3√ó speedup)
- [ ] **Mixed Precision**: BF16 for NS operations (1.5√ó speedup)
- [ ] **Adaptive Projection**: Dynamic threshold based on drift magnitude
- [ ] **Convergence Theory**: Formal analysis of projection error bounds

---

## üìö References

1. **Defazio, A., Mishchenko, K.** (2024). "The Road Less Scheduled: Removing Learning Rate Schedules with Primal-Averaging." *arXiv preprint*.

2. **Jordan, K., et al.** (2024). "Muon: Momentum Orthogonalized by Newton-Schulz Iteration." *FAIR Research*.

3. **Higham, N. J.** (1986). "Computing the Polar Decomposition‚Äîwith Applications." *SIAM J. Sci. Stat. Comput.*

4. **Edelman, A., Arias, T. A., Smith, S. T.** (1998). "The Geometry of Algorithms with Orthogonality Constraints." *SIAM J. Matrix Anal. Appl.*

---

## üéì Thesis Contribution

### What This Work Proves

‚úÖ **Geometric Soundness**: Projection scheme successfully recovers orthogonality 
from random initialization, achieving machine precision (4.2e-05)

‚úÖ **Practical Viability**: Optimizer converges on regression tasks while 
maintaining geometric constraints

‚úÖ **Novel Approach**: First application of Schedule-Free averaging to spectral 
optimizers with manifold projection

### What Remains to be Validated

‚è≥ **Computational Efficiency**: GPU benchmarks needed to confirm overhead is 
acceptable for production use

‚è≥ **Convergence Rate**: Comparison to AdamW on real datasets (WikiText-2, ImageNet) 
needed to validate step reduction claims

‚è≥ **Scalability**: Large-scale LLM training (7B+ parameters) required to prove 
production readiness

### Honest Assessment

This work provides a **mathematically sound foundation** for schedule-free 
optimization on Riemannian manifolds. The geometric approach is validated, 
but **extensive empirical validation on realistic tasks is needed** before 
claiming superiority to existing methods.

The contribution is in the **methodology** (deferred projection, drift tolerance) 
rather than proven performance gains.

---

## üéì Citation

If you use this code in your research, please cite:

```bibtex
@software{schedule_free_muon_2025,
  author = {PhD Research Team},
  title = {Schedule-Free Muon: Removing Hyperparameters from Matrix-Based Learning},
  year = {2025},
  url = {https://github.com/your-org/schedule-free-muon}
}
```

---

## üìÑ License

MIT License - see [LICENSE](LICENSE) file for details.

---

## ü§ù Contributing

This is an active research project. Contributions welcome:

1. **Bug Reports**: Open an issue with minimal reproducible example
2. **Feature Requests**: Describe use case and expected behavior
3. **Pull Requests**: Include tests and update documentation

---

## üìß Contact

**PhD Candidate**: [Your Name]  
**Advisor**: [Advisor Name]  
**Institution**: [University]  
**Email**: [your.email@university.edu]

---

## üôè Acknowledgments

- **FAIR (Facebook AI Research)** for Schedule-Free Learning
- **Jordan et al.** for Muon optimizer
- **Anthropic Research Team** for guidance on geometric optimization
- **[Funding Agency]** for financial support

---

**Status**: üöß **Research Prototype** - Not production-ready  
**Last Updated**: November 2025